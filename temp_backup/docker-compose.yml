version: '3.8'
services:

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  # Kafka Brokers
  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  mysql:
    image: mysql:8.0
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: smart_ads
    volumes:
      - ./mysql/init.sql:/docker-entrypoint-initdb.d/init.sql

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  spark:
    build: ./spark
    volumes:
      - ./spark/jobs:/app
      - ./data:/data
      - ./lakehouse:/lakehouse

  # airflow:
  #   build: ./airflow
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/bitnami/airflow/airflow.db
  #   volumes:
  #     - ./airflow/dags:/opt/bitnami/airflow/dags
  #   ports:
  #     - "8080:8080"


  # airflow:
  #   image: apache/airflow:2.7.3
  #   container_name: airflow
  #   restart: always
  #   ports:
  #     - "8080:8080"
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
  #   entrypoint: /bin/bash -c "airflow db init && airflow users create --username airflow --password airflow --firstname admin --lastname user --role Admin --email admin@example.com && airflow webserver"
  
  airflow-init:
    image: apache/airflow:2.7.3
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
      - airflow_db_volume:/opt/airflow  
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    entrypoint: /bin/bash -c "
      airflow db init &&
      airflow users create --username airflow --password airflow --firstname admin --lastname user --role Admin --email admin@example.com"

  airflow-web:
    image: apache/airflow:2.7.3
    container_name: airflow-web
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
      - airflow_db_volume:/opt/airflow  
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.7.3
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
      - airflow_db_volume:/opt/airflow  # SHARED volume for SQLite
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    command: scheduler

  producer:
    build: .
    command: python kafka_producer.py
    volumes:
      - ./scripts:/scripts
      - ./data:/data
    working_dir: /scripts
    depends_on:
      - kafka

  data_init:
    build: .
    command: python insert_data.py
    volumes:
      - ./scripts:/scripts
      - ./data:/data
    working_dir: /scripts
    depends_on:
      - mysql
      - redis

  adminer:
    image: adminer
    ports:
      - 8081:8080
    depends_on:
      - mysql

  redis_commander:
    image: rediscommander/redis-commander:latest
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports:
      - 8082:8081
    depends_on:
      - redis

  consumer:
    build: .
    command: python consumer_service.py
    volumes:
      - ./scripts:/scripts
    working_dir: /scripts
    depends_on:
      - kafka
      - redis
      - mysql
# Add to your docker-compose.yml
  metabase:
    image: metabase/metabase
    ports:
      - "3000:3000"
    environment:
      - MB_DB_TYPE=mysql
      - MB_DB_DBNAME=smart_ads
      - MB_DB_PORT=3306
      - MB_DB_USER=root
      - MB_DB_PASS=root
      - MB_DB_HOST=mysql
    depends_on:
      - mysql

volumes:
  airflow_db_volume:
  superset_db_data: